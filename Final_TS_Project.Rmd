---
title: "TimeSeries Walmart Project"
author: "Kenya Roy and Lani Lewis"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---

We invite you to watch our powerpoint presentation on our predictive modeling work online: https://youtu.be/eyyFiHjTxng

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

knitr::opts_chunk$set(warning = FALSE)

#install.packages("ggplot2")
library(ggplot2)

library(tswge)
library(forecast)

library(dplyr)
library(corrplot)


# Signal Plus Noise
#install.packages("orcutt")
#library(orcutt)

# Dickey Fuller Test
#install.packages("tseries")
library(tseries)

library(vars)

# Fancharts
library(RColorBrewer)

# MLP
#install.packages("nnfor")
library(nnfor)


# Load Files
library(tcltk)

# Function to show a popup and allow the user to choose a file
choose_file_with_message <- function(message) {
  tkmessageBox(message = message, icon = "info", type = "ok") # Display the message
  chosen_file <- tk_choose.files() # Open the file dialog
  return(chosen_file) # Return the chosen file path
}

# Use the function to choose files with descriptive messages
train <- read.csv(choose_file_with_message("Please select the 'train' file"))
feature <- read.csv(choose_file_with_message("Please select the 'feature' file"))
store <- read.csv(choose_file_with_message("Please select the 'store' file"))
valid <- read.csv(choose_file_with_message("Please select the 'test' file"))
```

# EDA & CLEAN DATA
## Merge Data
```{r, echo=FALSE, results='hide'}
new_trained = merge(train, feature, by = c("Store", "Date"), all.x = T) # Left Join tested and no Unmatched fields...

trained = merge(new_trained, store, by = c("Store"), all.x = T) 

head(trained)
```

### Validation Dataset
```{r, echo=FALSE, results='hide'}
new_validation = merge(valid, feature, by = c("Store", "Date"), all.x = T)

validation = merge(new_validation, store, by = c("Store"), all.x = T) 

head(validation)
```

### Clean up merge duplicate columns
```{r, echo=FALSE, results='hide'}
# Automatically drop duplicate columns with ".y" suffix
trained <- trained[, !grepl("\\.y$", names(trained))]
#test_set <- test_set[, !grepl("\\.y$", names(test_set))]
validation <- validation[, !grepl("\\.y$", names(validation))]

# Rename columns ending with ".x" by removing the ".x" suffix
names(trained) <- gsub("\\.x$", "", names(trained))
#names(test_set) <- gsub("\\.x$", "", names(test_set))
names(validation) <- gsub("\\.x$", "", names(validation))


# Confirm the dimensions of each dataset
cat("Training set dimensions:", dim(trained), "\n")
#cat("Test set dimensions:", dim(test_set), "\n")
cat("Validation set dimensions:", dim(validation), "\n")
```

### Clean up date types

**Training Dataset**
```{r, echo=FALSE, results='hide'}
# Check data types for both train_set and test_set (optional)
# sapply(train_set, class)
# sapply(test_set, class)

# Function to process a dataset (convert data types)
process_dataset <- function(data) {
  # Convert "Date" column to Date format
  data$Date <- as.Date(data$Date, format = "%Y-%m-%d")
  
  # Ensure IsHoliday remains as integers (0/1)
  data$IsHoliday <- as.integer(data$IsHoliday)
  
  # Convert numeric columns to integers where appropriate, excluding Weekly_Sales and IsHoliday
  numeric_columns <- setdiff(names(data)[sapply(data, is.numeric)], c("Weekly_Sales", "IsHoliday"))
  data[numeric_columns] <- lapply(data[numeric_columns], as.integer)
  
  # Keep "Weekly_Sales" and other continuous variables as numeric
  if ("Weekly_Sales" %in% names(data)) {
    data$Weekly_Sales <- as.numeric(data$Weekly_Sales)
  }
  
  # Return the processed dataset
  return(data)
}

# Apply the function to train_set and test_set
trained <- process_dataset(trained)


# Verify the updated data types
cat("Original Trained structure:\n")
str(trained)

```
**Validation Dataset**
```{r, echo=FALSE, results='hide'}
# Convert "Date" column to Date format
validation$Date <- as.Date(validation$Date, format = "%Y-%m-%d")

# Ensure IsHoliday remains as integers (0/1)
validation$IsHoliday <- as.integer(validation$IsHoliday)

# Convert numeric columns to integers where appropriate, excluding Weekly_Sales and IsHoliday
numeric_columns <- setdiff(names(validation)[sapply(validation, is.numeric)], "IsHoliday")
validation[numeric_columns] <- lapply(validation[numeric_columns], as.integer)

# Verify the updated data types
str(validation)
```


## NA Values
```{r, echo=FALSE}
# calculate the percentage of missing values for each column
missing_percentage <- sapply(trained, function(col) {
  sum(is.na(col)) / length(col) * 100
})

# Calculate the percentage of missing values for each column
missing_percentage <- sapply(trained, function(col) {
  sum(is.na(col)) / length(col) * 100
})

missing_percentage
```
### Remove NA >50%
```{r, echo=FALSE, results='hide'}
# Drop all columns with names starting with 'MarkDown'
trained <- trained[, !grepl("^MarkDown", names(trained))]

trained
```


### SPLIT

**Filtered Training Dataset**
```{r, echo=FALSE, results='hide'}
set.seed(123)

# Ensure the "Date" column is in Date format
trained$Date <- as.Date(trained$Date, format = "%Y-%m-%d")

# Filter the dataset to only include stores of Type 'A'
trained_filtered <- trained %>% 
  filter(Type == 'A')

trained_filtered
```
## Aggregate Weekly_Sales and retain other columns
```{r, echo=FALSE, results='hide'}
# Aggregate Weekly_Sales and retain other columns
aggregated_data <- trained_filtered %>%
  group_by(Date) %>%
  summarize(
    weekly_sales = sum(Weekly_Sales, na.rm = TRUE),  # Sum Weekly_Sales
    IsHoliday = first(IsHoliday),  # Retain the first IsHoliday value for each date
    Temperature = mean(Temperature, na.rm = TRUE),  # Average temperature for the date
    Fuel_Price = mean(Fuel_Price, na.rm = TRUE),    # Average fuel price
    CPI = mean(CPI, na.rm = TRUE),                  # Average CPI
    Unemployment = mean(Unemployment, na.rm = TRUE),# Average unemployment
    Store = n_distinct(Store),                      # Number of distinct stores (if applicable)
    .groups = "drop"                                # Ungroup after summarization
  )

# Sort data by Date
aggregated_data <- aggregated_data %>% arrange(Date)

# Check aggregated data
aggregated_data
```

## 80/20 Train Test Split
```{r, echo=FALSE}
# Split 80% for training and 20% for testing
split_index <- floor(0.8 * nrow(aggregated_data))
train_set <- aggregated_data[1:split_index, ]
test_set <- aggregated_data[(split_index + 1):nrow(aggregated_data), ]

# Confirm the dimensions and proportions
total_rows <- nrow(aggregated_data)
train_rows <- nrow(train_set)
test_rows <- nrow(test_set)

train_percentage <- (train_rows / total_rows) * 100
test_percentage <- (test_rows / total_rows) * 100

cat("Training set dimensions:", dim(train_set), "\n")
cat("Test set dimensions:", dim(test_set), "\n")
cat(sprintf("Training set percentage: %.2f%%\n", train_percentage))
cat(sprintf("Test set percentage: %.2f%%\n", test_percentage))
```

#### TEST SPLIT OF DATA
**Aggregated Date**
```{r, echo=FALSE}
table(format(aggregated_data$Date, "%Y"))
```

**Train Test Set**
```{r, echo=FALSE, results='hide'}
plot(train_set$Date,train_set$Weekly_Sales)
```

**Test Set**
- This is different then the validation set
```{r, echo=FALSE, results='hide'}
plot(test_set$Date,test_set$Weekly_Sales)
```

#### SCALE VARIABLES
**AGGREGATED**
```{r, echo=FALSE, results='hide'}
# Create a copy of your dataset to keep the original intact
scaled_a <- aggregated_data

# List of columns to scale
columns_to_scale <- c("Temperature", "Fuel_Price", "CPI", "Unemployment", "weekly_sales")

# Scale the columns and replace them in the dataset
scaled_a[columns_to_scale] <- scale(scaled_a[columns_to_scale])

# Check the scaled dataset
head(scaled_a)
```

**TRAINED**
```{r, echo=FALSE, results='hide'}
# Create a copy of your dataset to keep the original intact
scaled <- train_set

# List of columns to scale
columns_to_scale <- c("Temperature", "Fuel_Price", "CPI", "Unemployment", "weekly_sales")

# Scale the columns and replace them in the dataset
scaled[columns_to_scale] <- scale(scaled[columns_to_scale])

# Check the scaled dataset
head(scaled)
```

**TEST**
```{r, echo=FALSE, results='hide'}
# Create a copy of your dataset to keep the original intact
scaled_test <- test_set

# List of columns to scale
columns_to_scale <- c("Temperature", "Fuel_Price", "CPI", "Unemployment", "weekly_sales")

# Scale the columns and replace them in the dataset
scaled_test[columns_to_scale] <- scale(scaled_test[columns_to_scale])

# Check the scaled dataset
head(scaled_test)
```

### LOG VARIABLES
**AGGREGATED**
```{r, echo=FALSE, results='hide'}
# Apply log transformation to variables
scaled_a$Temperature_L <- log(aggregated_data$Temperature + 1)  # Add 1 to avoid log(0)
scaled_a$Fuel_Price_L <- log(aggregated_data$Fuel_Price + 1)

# If you want to log-transform the dependent variable (Weekly_Sales)
scaled_a$weekly_sales_L <- log(aggregated_data$weekly_sales + 1)
```

**TRAINED**
```{r, echo=FALSE, results='hide'}
# Apply log transformation to variables
scaled$Temperature_L <- log(train_set$Temperature + 1)  # Add 1 to avoid log(0)
scaled$Fuel_Price_L <- log(train_set$Fuel_Price + 1)

# If you want to log-transform the dependent variable (Weekly_Sales)
scaled$weekly_sales_L <- log(train_set$weekly_sales + 1)
```

**TEST**
```{r, echo=FALSE, results='hide'}
# Apply log transformation to variables
scaled_test$Temperature_L <- log(test_set$Temperature + 1)  # Add 1 to avoid log(0)
scaled_test$Fuel_Price_L <- log(test_set$Fuel_Price + 1)

# If you want to log-transform the dependent variable (Weekly_Sales)
scaled_test$weekly_sales_L <- log(test_set$weekly_sales + 1)

# Check the transformed dataset
head(scaled_test)
```


##### COVERT DATASETS TS 
```{r, echo=FALSE, results='hide'}
# Define training and testing datasets for monthly frequency
d12_train <- ts(scaled_a$weekly_sales[1:134], frequency = 12)  
d12_test <- ts(scaled_a$weekly_sales[135:143], frequency = 12)  

# Define training and testing datasets for weekly frequency
d52_train <- ts(scaled_a$weekly_sales[1:134], frequency = 52)  
d52_test <- ts(scaled_a$weekly_sales[135:143], frequency = 52)  

# For monthly frequency (12)
exog_L12 <- data.frame(
  #date = as.Date(scaled_a$Date),  
  cpi = as.numeric(scaled_a$CPI),
  holiday = as.numeric(scaled_a$IsHoliday),  
  temperature = as.numeric(scaled_a$Temperature_L), 
  fuel = as.numeric(scaled_a$Fuel_Price_L)  
)

# For weekly frequency (52)
exog_L52 <- data.frame(
  #date = as.Date(scaled_a$Date),  
  cpi = as.numeric(scaled_a$CPI),
  holiday = as.numeric(scaled_a$IsHoliday),  
  temperature = as.numeric(scaled_a$Temperature_L),  
  fuel = as.numeric(scaled_a$Fuel_Price_L)  
)


# For monthly frequency (12)
exog_12 <- data.frame(
  #date = as.Date(scaled_a$Date),  
  cpi = as.numeric(scaled_a$CPI),
  holiday = as.numeric(scaled_a$IsHoliday),  
  temperature = as.numeric(scaled_a$Temperature),  
  fuel = as.numeric(scaled_a$Fuel_Price)  
)

# For weekly frequency (52)
exog_52 <- data.frame(
  #date = as.Date(scaled_a$Date),  
  cpi = as.numeric(scaled_a$CPI),
  holiday = as.numeric(scaled_a$IsHoliday),  
  temperature = as.numeric(scaled_a$Temperature),  
  fuel = as.numeric(scaled_a$Fuel_Price)  
)
```

### DIFFERENCE DATA
```{r, echo=FALSE, results='hide'}
# Apply seasonal difference (lag = 52) and first-order difference (1-B)
diff = artrans.wge(scaled$weekly_sales, phi.tr = 1) 
 
#season = artrans.wge(diff, phi.tr = 52) 

season = artrans.wge(diff, phi.tr = c(rep(0,51),1)) 

monthly = artrans.wge(diff, phi.tr = c(rep(0,11),1)) 
```

## PLOTS
**Variables that are highly skewed often benefit from a log transformation.**
```{r, echo=FALSE, results='hide'}
# Plot histograms for all numeric variables
#numeric_vars <- sapply(train_set, is.numeric)  # Identify numeric columns
numeric_vars <- sapply(scaled, is.numeric)
#trained_numeric <- train_set[, numeric_vars]
trained_numeric <- scaled[, numeric_vars]

# Plot histograms
par(mfrow = c(2, 2))  # Create a grid for multiple plots
for (col in colnames(trained_numeric)) {
  hist(trained_numeric[[col]], main = paste("Histogram of", col), xlab = col)
}
```

**Look for non-linear relationships where a log transformation could help linearize the data.**
- Possibly log variables
```{r, echo=FALSE, results='hide'}
# Scatter plots of Weekly_Sales vs numeric variables
par(mfrow = c(2, 2))
for (col in setdiff(colnames(trained_numeric), "Weekly_Sales")) {
  #plot(trained_numeric[[col]], train_set$Weekly_Sales, 
    plot(trained_numeric[[col]], scaled$Weekly_Sales, 
       main = paste("Scatter Plot:", col, "vs Weekly_Sales"),
       xlab = col, ylab = "Weekly_Sales")
}
```

**May not need to use Highly correlated data**
- Keep all variables
```{r, echo=FALSE, results='hide'}
# Identify numeric columns
#numeric_vars <- sapply(train_set, is.numeric)
numeric_vars <- sapply(scaled, is.numeric)

# Calculate correlations for each numeric variable with Weekly_Sales
#correlations <- sapply(names(train_set)[numeric_vars], function(col) {
correlations <- sapply(names(scaled)[numeric_vars], function(col) {
  if (col != "Weekly_Sales") {  # Exclude Weekly_Sales from self-correlation
#    cor(train_set[[col]], train_set$weekly_sales, use = "complete.obs")
    cor(scaled[[col]], scaled$weekly_sales, use = "complete.obs")
  } else {
    NA  # Exclude Weekly_Sales from self-correlation
  }
})

# Convert to a data frame for better readability
correlation_df <- data.frame(
  Variable = names(correlations),
  Correlation = correlations
)

# Remove NA values (like self-correlation of Weekly_Sales)
correlation_df <- correlation_df[!is.na(correlation_df$Correlation), ]

# Sort by absolute correlation values
correlation_df <- correlation_df[order(abs(correlation_df$Correlation), decreasing = TRUE), ]

# Display the results
print(correlation_df)
```
##Correlation Matrix of Numeric Variables
Unemployment has high correlation with Fuel_Price and CPI variables. 
```{r}
numeric_vars <- train_set %>%
  select_if(is.numeric) # Select all numeric columns

# drop weekly_sales
numeric_vars <- numeric_vars[, !colnames(numeric_vars) %in% "weekly_sales"]
numeric_vars <- numeric_vars[, !colnames(numeric_vars) %in% "Store"]
numeric_vars <- numeric_vars[, !colnames(numeric_vars) %in% "Dept"]
numeric_vars <- numeric_vars[, !colnames(numeric_vars) %in% "Size"]

# Compute the correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")  # Use only complete cases

# Print the correlation matrix
print(cor_matrix)

# Adjust margins to create space for the title
par(mar = c(5, 5, 7, 5))  # Increase the top margin (3rd value)
corrplot(cor_matrix, 
         method = "color", 
         type = "upper", 
         tl.col = "black", 
         tl.srt = 45)
title("Correlation Matrix of Numeric Variables", line = 5)  # Add title with proper spacing

```

### TS Plots
- Realization: The sharp spikes and upward or downward trends suggest that the series is non-stationary (mean and variance are not constant over time).
- ACF: Strong correlations at lag 1 and lag 2, gradually decreasing, also indicating non-stationarity.
- Spectral Density: looks like we have three high peaks the first is the strongest and we see high peaks around period 12 (13/14)
 - The next import periods are around 3 then 2...
 - The overall behavior indicates that differencing might be required to stabilize the series.
```{r, echo=FALSE, results='hide'}

#plotts.sample.wge(train_set$weekly_sales, arlimits = T)
plotts.sample.wge(scaled$weekly_sales, arlimits = T)
```

#### ACF | MA(q) Review
- Maybe MA(4)
```{r, echo=FALSE, results='hide'}
#acf(train_set$weekly_sales)
acf(scaled$weekly_sales)
```

#### PACF | AR(p) Review
- maybe a AR(4)
 - scaled it looks like an AR(2)
```{r, echo=FALSE, results='hide'}
#pacf(train_set$weekly_sales)
pacf(scaled$weekly_sales)
```

#### Plot for Trend
- It looks like there are spike around 13(11 - Nov) and 14(12 - Dec) period.
 - Maybe a frequency peak at around .071 - .077 
```{r, echo=FALSE, results='hide'}
#plot(train_set$Date, train_set$weekly_sales, type = "l", main = "Weekly Sales Over Time", xlab = "Date", ylab = "Sales")

plot(scaled$Date, scaled$weekly_sales, type = "l", main = "Weekly Sales Over Time", xlab = "Date", ylab = "Sales")
```

##### Decompose Time Series
**Summary of Insights**
- **Trend**: A clear upward trend is present between time 2 and 3, indicating long-term growth.
- **Seasonality**: The data has a strong seasonal component, which should be explicitly modeled.
- **Stationarity**: The presence of both trend and seasonality suggests the series is non-stationary. First-order differencing (`d=1`) and/or seasonal differencing (`D=1`) may be required to stabilize the series for ARIMA modeling.
- **Outliers**: The sharp spikes in the residuals suggest potential outliers or irregular events that may require further investigation or adjustments.
---
```{r, echo=FALSE, results='hide'}
# Convert data to a time series object
#sales_ts <- ts(train_set$weekly_sales, frequency = 52)  # Weekly data, 52 weeks per year
sales_ts <- ts(scaled$weekly_sales, frequency = 52)

# Decompose the series
decomposition <- decompose(sales_ts, type = "additive")  # or "multiplicative"
plot(decomposition)
```


#### Dickey-Fuller Test for Stationarity
- Reject the null hypothesis of Non-Stationarity
 - This test was not as helpful as I had hopped cause we know that we need to difference the model from the plots above.
 - This is actually a non-stationary model based off Kaggle as well. On the site they already mention this is a non-stationary model
```{r, echo=FALSE, results='hide'}
#adf_test <- adf.test(train_set$weekly_sales)
adf_test <- adf.test(scaled$weekly_sales)
print(adf_test)
```

# ARMA MODEL

## AIC Model Determination
- I will test the following three models
  - ARMA(5,1) increase the values for more options since we are at the max
    - ARMA(5,1) is still in the top 5 so I will try this option
    - ARMA(5,3) and ARMA(6,1) will be the next I play with.
- After testing I found that ARMA(5,3) performs the best
```{r, echo=FALSE}
#aic5.wge(train_set$weekly_sales, p=0:7, q=0:3)
aic5.wge(scaled$weekly_sales, p=0:7, q=0:3)
```

## BIC Model Determination
- Not going to use this model cause we know this is not white noise. So I increased the model
 - Still not valid options
```{r, echo=FALSE}
#aic5.wge(train_set$Weekly_Sales, p=0:7, q=0:3, type = 'bic')
aic5.wge(scaled$Weekly_Sales, p=0:7, q=0:3, type = 'bic')
```

## ARMA(5,3) Factor Table Check | Model Estimates
- Here the 1-B is very apparent. This might be the best model to use since I know I need to difference my model
- The frequency also seems to match up almost exactly with what I am seeing in the original realization
```{r, echo=FALSE}
#arma53 = est.arma.wge(train_set$weekly_sales, p = 5, q=3)
arma53 = est.arma.wge(scaled$weekly_sales, p = 5, q=3)
```

### RESIDUAL CHECKS ARMA(5,3)

#### ACF Residual Check
- Passed the ACF White Noise Residual Check
```{r, echo=FALSE, results='hide'}
acf(arma53$res)
```

#### Ljung Residual Check
- Fail to reject the Null Hypothesis of White Noise as the p-value is greater then .05 for both tests
 - This is another pass for this model
```{r, include=FALSE}
arma_l1 = ljung.wge(arma53$res, p=5, q=3)
```
```{r, echo=FALSE}
cat("p-value",arma_l1$pval)
```

```{r, include=FALSE}
arma_l2 = ljung.wge(arma53$res, p=5, q=3, K=48)
```
```{r, echo=FALSE}
cat("p-value",arma_l2$pval)
```

## Forecast ARMA Model
```{r, echo=FALSE, results='hide'}
#arma_for = fore.arma.wge(train_set$weekly_sales, phi = arma53$phi, theta = arma53$theta, n.ahead = 8, lastn = T, limits = T)

arma_for = fore.arma.wge(scaled$weekly_sales, phi = arma53$phi, theta = arma53$theta, n.ahead = 8, lastn = T, limits = T)
```

### ASE
```{r, echo=FALSE}
#arma_ase = mean((test_set$weekly_sales - arma_for$f)^2)
arma_ase = mean((scaled_test$weekly_sales - arma_for$f)^2)

arma_ase
```
### WMAE | Kaggle
```{r, echo=FALSE}
weights <- rep(1, length(scaled_test$weekly_sales))  

# Calculate WMAE
arma_wmae <- sum(weights * abs(scaled_test$weekly_sales - arma_for$f)) / sum(weights)

# Print the result
arma_wmae
```

### Compare Multiple Spectral Densities
- This model appears to perform well with generating the spectral densities
```{r, echo=FALSE, results='hide'}
sims = 10
#SpecDen = parzen.wge(train_set$weekly_sales, plot = "FALSE")
SpecDen = parzen.wge(scaled$weekly_sales, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6) # Best look at the model

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.arima.wge(114, phi = arma53$phi,theta = arma53$theta, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red") 
} # 30 examples of the spectral data to compare to the original
```

### Compare Multiple ACFs
- Does fairly well modeling the ACFs
```{r, echo=FALSE, results='hide'}
sims = 10
#ACF = acf(train_set$weekly_sales, plot = "FALSE")
ACF = acf(scaled$weekly_sales, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

for( i in 1: sims)
{
   ACF2 = acf(gen.arima.wge(114, phi = arma53$phi,theta = arma53$theta, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}
```

## FINAL ARMA(5,3) MODEL
$$(1 - 0.1845B - 0.5098B^2 - 0.3896B^3 - 0.0227B^4 + 0.3018B^5)(X_t + 4.470451e-16) = (1 + 0.1845B - 0.4290B^2 - 0.7555B^3)a_t, \ \hat{\sigma}^2_a = 0.6783202$$

# ARIMA (5,1,3)

## Dickey-Fuller Test for Stationarity
- Reject the null hypothesis of Non-Stationarity
```{r, echo=FALSE, results='hide'}
adf_test <- adf.test(diff)
print(adf_test)
```
## AIC Model Determineation
- We don't see ARMA(5,3) in the top options but decided to move forward with this option do the results from the first model
```{r, echo=FALSE}
aic5.wge(diff, p=0:7, q=0:3)
```

## BIC Model Determination
```{r, echo=FALSE}
aic5.wge(diff, p=0:7, q=0:3, type = "bic")
```

## ARIMA (5,1,3) Factor Table Check | Model Estimates
- Frequencies don't match exactly with the original model as the dominance has change
- We still see the 1-B in the Moving Average Model
```{r, echo=FALSE}
diff_est = est.arma.wge(diff, p=5, q=3)
```

### RESIDUAL CHECKS ARIMA (5,1,3) 

#### ACF Residual Check
- Passed the ACF White Noise Residual Check
```{r, echo=FALSE, results='hide'}
acf(diff_est$res)
```

#### Ljung Residual Check
- Fail to reject the Null Hypothesis of White Noise as the p-value is greater then .05 for both tests
 - This is another pass for this model
```{r, include=FALSE}
season_l1 = ljung.wge(diff_est$res, p=5, q=3)
```
```{r, echo=FALSE}
cat("p-value",season_l1$pval)
```

```{r, include=FALSE}
season_l2 = ljung.wge(diff_est$res, p=5, q=3, K=48)
```
```{r, echo=FALSE}
cat("p-value",season_l2$pval)
```

## Forecast ARIMA (5,1,3) Model
```{r, echo=FALSE, results='hide'}
diff_for = fore.aruma.wge(scaled$weekly_sales, phi = diff_est$phi, theta = diff_est$theta, d=1, n.ahead = 8, lastn = T, limits = T)
title(main = "Forecast ARIMA (5,1,3) Model")
```

#### ASE
```{r, echo=FALSE}
diff_ase = mean((scaled_test$weekly_sales - diff_for$f)^2)

diff_ase 
```

### WMAE | Kaggle
```{r, echo=FALSE}
weights <- rep(1, length(scaled_test$weekly_sales))  

# Calculate WMAE
diff_wmae <- sum(weights * abs(scaled_test$weekly_sales - diff_for$f)) / sum(weights)

# Print the result
diff_wmae
```

#### Compare Multiple Spectral Densities
- This model appears to perform well with generating the spectral densities
```{r, echo=FALSE, results='hide'}
sims = 50
SpecDen = parzen.wge(train_set$weekly_sales, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6, main = "ARIMA(5,1,3) Multiple Spectral Densities Comparison")

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.aruma.wge(114, phi = diff_est$phi,theta = diff_est$theta, d=1, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red") 
} # 30 examples of the spectral data to compare to the original
```

#### Compare Multiple ACFs
- Does fairly well modeling the ACFs
```{r, echo=FALSE, results='hide'}
sims = 50
ACF = acf(train_set$weekly_sales, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6, main = "ARIMA(5,1,3) ACF Comparison")

for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(114, phi = diff_est$phi,theta = diff_est$theta, d=1, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}
```

## FINAL ARIMA (5,1,3) MODEL  
$$(1 - 0.2202B + 0.0766B^2 - 0.0163B^3 - 0.2829B^4 + 0.3569B^5)(1 - B)(X_t - 0.006105923) = (1 - 0.8525B + 0.0815B^2 - 0.2290B^3)a_t, \ \hat{\sigma}^2_a = 0.7413429$$

# ARUMA(3,1,0) w/ s=52

## Dickey-Fuller Test for Stationarity
- Reject the null hypothesis of Non-Stationarity
```{r, echo=FALSE, results='hide'}
adf_test <- adf.test(season)
print(adf_test)
```

## AIC Model Determineation
```{r, echo=FALSE}
aic5.wge(season, p=0:7, q=0:3)

#aic5.wge(monthly, p=0:7, q=0:3)
```

## BIC Model Determineation
```{r, echo=FALSE}
aic5.wge(season, p=0:7, q=0:3, type = "bic")

#aic5.wge(monthly, p=0:7, q=0:3, type = "bic")
```

## ARUMA(3,1,0) w/ s = 52 Factor Table Check | Model Estimates
```{r, echo=FALSE}
season_est = est.arma.wge(season, p=3, q=0)

#monthly_est = est.arma.wge(season, p=6, q=1)
```

### RESIDUAL CHECKS ARUMA(3,1,0) w/ s=52

#### ACF Residual Check
- Passed the ACF White Noise Residual Check
```{r, echo=FALSE, results='hide'}
acf(season_est$res)
```

#### Ljung Residual Check
- Fail to reject the Null Hypothesis of White Noise as the p-value is greater then .05 for both tests
 - This is another pass for this model
```{r, include=FALSE}
season_l1 = ljung.wge(season_est$res, p=3, q=0)
```
```{r, echo=FALSE}
cat("p-value",season_l1$pval)
```

```{r, include=FALSE}
season_l2 = ljung.wge(season_est$res, p=3, q=0, K=48)
```
```{r, echo=FALSE}
cat("p-value",season_l2$pval)
```

## Forecast ARUMA(3,1,0) w/ s=52 Model
```{r, echo=FALSE, results='hide'}
season_for = fore.aruma.wge(scaled$weekly_sales, phi = season_est$phi, theta = season_est$theta, d=1, s=52, n.ahead = 8, lastn = T, limits = T)
title(main = "Forecast ARUMA (3,1,0) Model")

#monthly_for = fore.aruma.wge(scaled$weekly_sales, phi = monthly_est$phi, theta = monthly_est$theta, d=1, s=12, n.ahead = 8, lastn = T, limits = T)
```

#### ASE
```{r, echo=FALSE}
season_ase = mean((scaled_test$weekly_sales - season_for$f)^2)


season_ase 
```

### WMAE | Kaggle
```{r, echo=FALSE}
weights <- rep(1, length(scaled_test$weekly_sales))  

# Calculate WMAE
season_wmae <- sum(weights * abs(scaled_test$weekly_sales - season_for$f)) / sum(weights)

# Print the result
season_wmae
```

#### Compare Multiple Spectral Densities
- This model does not appear to perform very well with generating the spectral densities
```{r, echo=FALSE, results='hide'}
sims = 50
SpecDen = parzen.wge(train_set$weekly_sales, plot = "FALSE")
plot(SpecDen$freq,SpecDen$pzgram, type = "l", lwd = 6) # Best look at the model

for( i in 1: sims)
{
   SpecDen2 = parzen.wge(gen.aruma.wge(114, phi = season_est$phi,theta = season_est$theta, d=1, s=52, plot ="FALSE"), plot = "FALSE")
   lines(SpecDen2$freq,SpecDen2$pzgram, lwd = 2, col = "red") 
} # 30 examples of the spectral data to compare to the original
```

#### Compare Multiple ACFs
- Doesn't do very well modeling the ACFs
```{r, echo=FALSE, results='hide'}
sims = 50
ACF = acf(train_set$weekly_sales, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(114, phi = season_est$phi,theta = season_est$theta, d=1, s=52, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}
```

## FINAL ARUMA (3,1,0) MODEL with s=52 

$$(1 + 0.5804B + 0.4008B^2 + 0.3096B^3)(1 - B)(X_t - 0.1205291) = a_t, \ \hat{\sigma}^2_a = 0.1205291 $$

# Signal Plus Noise

## Forecast Scaled Signal Plus Noise as Best Model

```{r, echo=FALSE, results='hide'}
#spn_for = fore.sigplusnoise.wge(train_set$weekly_sales, linear = T, max.p = 6, n.ahead = 8, lastn = T, limits = T) # ASE: 4.790823e+12

#spn_for = fore.sigplusnoise.wge(scaled$weekly_sales_L, linear = T, max.p = 6, n.ahead = 8, lastn = T, limits = T) # ASE: 9.038697e+14

spn_for = fore.sigplusnoise.wge(scaled$weekly_sales, linear = T, max.p = 7, n.ahead = 8, lastn = T, limits = T) 
par(mar = c(5, 4, -.75, 2) + 1.5)  
title(main = "Forecast Signal Plus Noise Model")
```

### ASE
```{r, echo=FALSE}
spn_ase = mean((scaled_test$weekly_sales - spn_for$f)^2)

spn_ase
```

### WMAE | Kaggle
```{r, echo=FALSE}
weights <- rep(1, length(scaled_test$weekly_sales))  # Equal weights; replace with actual weights if available

# Calculate WMAE
spn_wmae <- sum(weights * abs(scaled_test$weekly_sales - spn_for$f)) / sum(weights)

# Print the result
spn_wmae
```

## FINAL SIGNAL PLUS NOISE REGRESSION EQUATION
- Sales tend to decrease overtime

$$sales = -0.1896494 + .003298251(time)$$

# ARUMA MULTIVARIATE MODEL

## Cross-Correlation Plots
- In this case we are looking for the longest lag to determine how to lag multivariate models 
- Since we know Holiday is important and our lags are across the board, we decided to keep it simple and lag everything by 1
```{r, echo=FALSE}
#ccf(scaled$Date, scaled$CPI) # Heavily Correlated
```

```{r, echo=FALSE}
ccf(scaled$weekly_sales, scaled$CPI) # lag -15

ccf(scaled$weekly_sales, scaled$Date)  # lag -15

ccf(scaled$weekly_sales, scaled$Temperature) # lag -7

ccf(scaled$weekly_sales, scaled$Temperature_L) # lag -7

ccf(scaled$weekly_sales, scaled$Fuel_Price) # lag -15

ccf(scaled$weekly_sales, scaled$Fuel_Price_L) # lag -15

ccf(scaled$weekly_sales, scaled$IsHoliday) # lag -1

ccf(scaled$weekly_sales, scaled$Unemployment) # lag - 15
```

## Lag Variables
```{r, echo=FALSE, results='hide'}
scaled$temp1 = dplyr::lag(scaled$Temperature, 1)  
scaled$fuel_price1 = dplyr::lag(scaled$Fuel_Price, 1)
scaled$CPI1 = dplyr::lag(scaled$CPI, 1)
scaled$Unemployment1 = dplyr::lag(scaled$Unemployment, 1)
```

## Fit Regression Model
- will use this as the base line for the multivariate model
```{r, echo=FALSE, results='hide'}
ksfit = lm(scaled$weekly_sales[2:114] ~ scaled$temp1[2:114] + scaled$fuel_price1[2:114] +  scaled$Unemployment1[2:114] + scaled$CPI1[2:114]) 

summary(ksfit)
```

### Check Regression Residual
- Ensure the residuals have been whitened
```{r, echo=FALSE, results='hide'}
# Step 2 Review Residual
plotts.sample.wge(ksfit$residuals)
```

### Difference Regression Residuals
```{r, echo=FALSE, results='hide'}
res_diff52 = artrans.wge(ksfit$residuals, phi.tr = c(rep(0,51),1)) # 52 - n
```

### Model Regression Residual
**AIC**
```{r, echo=FALSE, results='hide'}
aic5.wge(res_diff52)
```

**BIC**
```{r, echo=FALSE, results='hide'}
aic5.wge(res_diff52, type = "bic")
```

## Forecast ARUMA (1,0,0) w/ s=52
```{r, echo=FALSE, results='hide'}
time = seq(1,114,1)

# Fit the ARIMA model
fit_lag <- arima(
  
  x = scaled$weekly_sales[2:114],
  order = c(1, 0, 0), 
  seasonal = list(order=c(0,1,0),period = 52),  # P,D,Q
  xreg = cbind( #reordered from VAR
  scaled$fuel_price1[2:114],
  scaled$CPI1[2:114],
  scaled$temp1[2:114],
  scaled$Unemployment1[2:114]
)
  
)

fit_lag
plotts.sample.wge(fit_lag$residuals)

```


#### ACF Residual Check
- Passed the ACF White Noise Residual Check
```{r, echo=FALSE, results='hide'}
acf(fit_lag$res)
```

#### Ljung Residual Check
- Fail to reject the Null Hypothesis of White Noise as the p-value is greater then .05 for both tests
 - This is another pass for this model
```{r, include=FALSE}
arimaU_l1 = ljung.wge(fit_lag$res, p=1, q=0)
```
```{r, echo=FALSE}
cat("p-value",arimaU_l1$pval)
```

```{r, include=FALSE}
arimau_l2 = ljung.wge(fit_lag$res, p=1, q=0, K=48)
```
```{r, echo=FALSE}
cat("p-value",season_l2$pval)
```

## MULTIVARIATE ARUMA(1,0,0) w/ s=52 Predictions
```{r, echo=FALSE, results='hide'}
last8 = data.frame(
  fuel_price1L = scaled$fuel_price1[107:114],
  CPI1L = scaled$CPI1[107:114],
  temp1L = scaled$temp1[107:114],
  unemployment1L = scaled$Unemployment1[107:114]
)

#get predictions
aruma_preds = predict(fit_lag,newxreg = last8)


weekly = aruma_preds$pred


plot(seq(1,114,1), scaled$weekly_sales, type = "l", xlim = c(0,115))
lines(seq(107,114,1), weekly, type = "l", col = "red")
title(main= "ARUMA(1,0,0) s=52 Forecasts")
```


### ASE
```{r, echo=FALSE}
aruma_ASE = mean((scaled$weekly_sales[107:114] - aruma_preds$pred)^2)
aruma_ASE 
```

### WMAE | Kaggle
```{r, echo=FALSE}
actual_last8 = scaled_test$weekly_sales[(length(scaled_test$weekly_sales)-7):length(scaled_test$weekly_sales)]

weights <- rep(1, length(actual_last8))  

# Calculate WMAE
ARUMA_wmae <- sum(weights * abs(actual_last8 - aruma_preds$pred)) / sum(weights)

ARUMA_wmae 
```

#### Compare Multiple Spectral Densities
- This model does not appear to perform well with generating the spectral densities

```{r, echo=FALSE}
sims = 50
SpecDen = parzen.wge(train_set$weekly_sales, plot = FALSE)
plot(SpecDen$freq, SpecDen$pzgram, type = "l", lwd = 6)

# Extract AR and MA coefficients
ar_coef = fit_lag$coef[1]  
ma_coef = fit_lag$coef[2]  

for (i in 1:sims) {
  SpecDen2 = parzen.wge(gen.aruma.wge(114, phi = ar_coef, theta = ma_coef, d = 1, s = 52, plot = FALSE), plot = FALSE)
  lines(SpecDen2$freq, SpecDen2$pzgram, lwd = 2, col = "red")
}
```

#### Compare Multiple ACFs
- Does not do fairly well modeling the ACFs
```{r, echo=FALSE, results='hide'}
sims = 50
ACF = acf(train_set$weekly_sales, plot = "FALSE")
plot(ACF$lag ,ACF$acf , type = "l", lwd = 6)

# Extract AR and MA coefficients
ar_coef = fit_lag$coef[1] 
ma_coef = fit_lag$coef[2]  


for( i in 1: sims)
{
   ACF2 = acf(gen.aruma.wge(114, phi = ar_coef, theta = ma_coef, d=1, s=52, plot = "FALSE"), plot = "FALSE")
   lines(ACF2$lag ,ACF2$acf, lwd = 2, col = "red")
}
```

## FINAL ARUMA(1,0,0) w/ s=52 MODEL
$$sales = (1 - 0.1780B) - 0.0665(lag.fuel) + 0.2753(lag.cpi) + 0.0083(lag.temp) + 0.1352(lag.unemployment)$$

# VAR Model
```{r, echo=FALSE}
set.seed(123)

xregs = cbind(
    #scaled$weekly_sales,
    #scaled$Date[3:114],
    #scaled$IsHoliday[3:114],
    scaled$Temperature,
    scaled$Fuel_Price,
    scaled$CPI)
    #scaled$Unemployment[3:114])

VARselect(scaled$weekly_sales, lag.max = 7, type = 'both', exogen = xregs)
```


## VAR Model Lag Predictions | Model Estimates
```{r, echo=FALSE,  include=FALSE}
set.seed(123)
sales = scaled$weekly_sales
isholiday =  scaled$IsHoliday
fuel_price = scaled$Fuel_Price
temperature = scaled$Temperature
cpi =     scaled$CPI
unemployment =  scaled$Unemployment
date_ = scaled$Date

# Seasonal Dummies?? | Season = 52
scaled$Week = as.factor(scaled$weekly_sales%%52)
week = scaled$Week

scaled$Month = as.factor(scaled$weekly_sales%%12)
month = scaled$Month


y = cbind(
    sales,
    fuel_price,
    temperature,
    week, # Seasonal Dummy
    cpi) # y3

lsfit = VAR(y, p=5, type = "both", season = 52) # p = lag in this case

summary(lsfit)
```

## Forecast VAR(5)
```{r, echo=FALSE}
var_pred = predict(lsfit, n.ahead = 8)

var_pred

weekly = var_pred$fcst$sales[,1]


plot(seq(1,114,1), scaled$weekly_sales, type = "l", xlim = c(0,115))
lines(seq(107,114,1), weekly, type = "l", col = "red")
title(main= "VAR Forecasts")
```

#### Confidence Interval 
```{r, echo=FALSE}
# Extract the lower and upper CI
wk_lower_ci <- quantile(var_pred$fcst$sales[, 1], probs = .975) # quantile(holder[,1],probs = .975)
wk_upper_ci <- quantile(var_pred$fcst$sales[, 1], probs = .025) #quantile(holder[,1],probs = .025)

# Use summary() to get statistics
#summary(wk_lower_ci)
#summary(wk_upper_ci)

# Calculate the averages
wk_average_lower <- mean(wk_lower_ci, na.rm = TRUE)
wk_average_upper <- mean(wk_upper_ci, na.rm = TRUE)

# SWITCH VALUES AROUND DUE TO NEGATIVE FORECASTS...
cat("Average Lower CI:", wk_average_upper, "\n") 
cat("Average Upper CI:", wk_average_lower, "\n")
```

#### ASE
```{r, echo=FALSE}
var_ASE = mean((scaled$weekly_sales[107:114] - weekly)^2)

var_ASE 
```

#### WMAE | Kaggle 
```{r, echo=FALSE}
weights <- rep(1, length(scaled_test$weekly_sales))  

actual_values <- scaled$weekly_sales[107:114]
predicted_values <- weekly

# Calculate WMAE
var_wmae <- sum(weights * abs(actual_values - predicted_values)) / sum(weights)

# Print the result
var_wmae
```

## FINAL VAR Regression Model

### Sales Equation:
$$
sales_t = -0.6228 + 0.1681 \cdot sales_{t-1} + 0.1818 \cdot fuel\_price_{t-1} - 0.1168 \cdot temperature_{t-1} + 0.0020 \cdot week_{t-1} + 0.4253 \cdot cpi_{t-1} \\
\quad - 0.1340 \cdot sales_{t-2} - 0.1005 \cdot fuel\_price_{t-2} - 0.1535 \cdot temperature_{t-2} + 0.0028 \cdot week_{t-2} - 0.7972 \cdot cpi_{t-2} + \ldots + \epsilon_t
$$


### Fuel Price Equation:
$$
fuel\_price_t = -0.9297 + 0.4673 \cdot fuel\_price_{t-1} - 0.0971 \cdot sales_{t-1} - 0.4713 \cdot cpi_{t-1} - 0.5961 \cdot temperature_{t-2} \\
\quad + 0.2138 \cdot fuel\_price_{t-2} + \ldots + \epsilon_t
$$


### Temperature Equation:
$$
temperature_t = -0.3104 + 0.3282 \cdot temperature_{t-1} + 0.6624 \cdot cpi_{t-1} - 0.5386 \cdot fuel\_price_{t-2} - 0.7805 \cdot cpi_{t-2} + \ldots + \epsilon_t
$$


### Week Equation:
$$
week_t = 153.9 - 0.236 \cdot week_{t-1} - 0.3757 \cdot week_{t-2} - 43.67 \cdot sales_{t-4} - 0.6056 \cdot week_{t-3} + \ldots + \epsilon_t
$$


### CPI Equation:
$$
cpi_t = -0.2888 + 0.6613 \cdot cpi_{t-1} + 0.0073 \cdot trend - 0.0313 \cdot fuel\_price_{t-4} - 0.1419 \cdot temperature_{t-4} + \ldots + \epsilon_t
$$
### Covariance Matrix of Residuals:
$$
\begin{bmatrix}
 0.0768 & -0.0054 & -0.0035 & -2.3978 & -0.0004 \\
-0.0054 &  0.0316 &  0.0029 & -0.6382 & -0.0010 \\
-0.0035 &  0.0029 &  0.0180 &  0.6042 & -0.0010 \\
-2.3978 & -0.6382 &  0.6042 & 500.7604 & 0.1007 \\
-0.0004 & -0.0010 & -0.0010 &  0.1007 & 0.0083
\end{bmatrix}
$$

### Correlation Matrix of Residuals:
$$
\begin{bmatrix}
 1.0000 & -0.1097 & -0.0933 & -0.3866 & -0.0153 \\
-0.1097 &  1.0000 &  0.1207 & -0.1604 & -0.0600 \\
-0.0933 &  0.1207 &  1.0000 &  0.2012 & -0.0814 \\
-0.3866 & -0.1604 &  0.2012 &  1.0000 &  0.0494 \\
-0.0153 & -0.0600 & -0.0814 &  0.0494 &  1.0000
\end{bmatrix}
$$


# MLP Model
```{r, echo=FALSE}
set.seed(123)

mlp52 = mlp(d52_train,reps = 10, comb = "median", difforder = c(1,12), allow.det.season = T, hd.auto.type = "cv", sel.lag = T, xreg = exog_52) 

mlp52
```

## PLOT MLP Model
```{r, echo=FALSE, results='hide'}
plot(mlp52)
```


## MLP Forecast
```{r, echo=FALSE}
mlp52_for = forecast(mlp52, h=8, xreg = exog_52)

mlp52_for

plot(mlp52_for)
```

#### ASE
```{r, echo=FALSE}
#mlp52_ase = mean((d52_test - mlp52_for$fitted)^2)
aligned_fitted <- mlp52_for$fitted[1:9]

# Recalculate ASE
mlp52_ase <- mean((d52_test - aligned_fitted)^2, na.rm = TRUE)

mlp52_ase # L: 0.1026821 (s = 12)
```

#### WMAE | Kaggle
```{r, echo=FALSE}
# Weight vector
weights <- rep(1, length(d52_test)) 

# Calculate WMAE
mlp52_wmae <- sum(weights * abs(d52_test - aligned_fitted)) / sum(weights)

# Print the result
mlp52_wmae 
```

# Ensemble - MLP and VAR
```{r, echo=FALSE, results='hide'}

  # Calculate the ensemble predictions
  ensemble = (mlp52_for$mean + weekly)/2  #MLP and VAR


# Print the ensemble predictions
print(ensemble)

plot(scaled$weekly_sales, type = "l")
lines(seq(107,114,1),ensemble,col = "green")
title("Ensemble Forecasts")

```
## ASE Ensemble
```{r, echo=FALSE}
ensemble_ASE = mean((scaled$weekly_sales[107:114] - ensemble)^2) 


ensemble_ASE
```


# Model Decision - ASE
```{r, echo=FALSE, results='hide'}
# Creating a data frame to hold the model results
# Creating a data frame to hold the model results
model_ase_results <- data.frame(
  Model = c("ARMA(5,3)", "ARIMA(5,1,3)", "ARUMA(3,1,0) s=52", "Signal+Noise", "Multivariate(1,0,0) s=52", "VAR(5)", "MLP", "Ensemble"), 
  ASE = c(arma_ase, diff_ase, season_ase, spn_ase, aruma_ASE, var_ASE, mlp52_ase, ensemble_ASE)
)

# Sort the data frame by ASE from smallest to largest
model_ase_results <- model_ase_results[order(model_ase_results$ASE), ]

# Display the sorted data frame
print(model_ase_results)
```



# Model Decision - WMAE
```{r, echo=FALSE, results='hide'}
# Creating a data frame to hold the model results
model_wmae_results <- data.frame(
  Model = c("ARMA(5,3)", "VAR(5)", "MLP", "Multivariate(1,0,0) s=52"), 
  WMAE = c(arma_wmae, var_wmae, mlp52_wmae, ARUMA_wmae)
)

# Sort the data frame by WMAE from smallest to largest
model_wmae_results <- model_wmae_results[order(model_wmae_results$WMAE), ]

# Display the sorted data frame
print(model_wmae_results)
```


# Model Comparison Barchart
```{r, echo=FALSE, results='hide'}
# Reorder Model levels based on ASE values (smallest to largest)
model_ase_results$Model <- factor(model_ase_results$Model, levels = model_ase_results$Model[order(model_ase_results$ASE)])

# Plotting the reordered ASE bar chart
ggplot(model_ase_results, aes(x = Model, y = ASE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.5) +
  labs(title = "Model ASE Comparison", x = "Model", y = "Lower (ASE) is Better") +
  theme_minimal() +
  ylim(0, 1.3)

# Reorder Model levels based on WMAE values (smallest to largest)
model_wmae_results$Model <- factor(model_wmae_results$Model, levels = model_wmae_results$Model[order(model_wmae_results$WMAE)])

# Plotting the reordered WMAE bar chart
ggplot(model_wmae_results, aes(x = Model, y = WMAE, fill = Model)) +
  geom_bar(stat = "identity", width = 0.5) +
  labs(title = "Model WMAE Comparison", x = "Model", y = "Lower (WMAE) is Better") +
  theme_minimal() +
  ylim(0, 1.3)
```
